# fizzbuzz tensor style

import numpy as np
import tensorflow as tf

NUM_DIGITS = 10

# represent each digit by an array of its binary digits.
def binary_encode(i, num_digits):
    return np.array([i >> d & 1 for d in range(num_digits)])

# one hot encode results: [number, "fizz", "buzz", "fizzbuzz"]
def fizz_buzz_encode(i):
    if i % 15 == 0: return np.array([0, 0, 0, 1])
    elif i % 5  == 0: return np.array([0, 0, 1, 0])
    elif i % 3  == 0: return np.array([0, 1, 0, 0])
    else: return np.array([1, 0, 0, 0])

# see one hot encoder in action
[fizz_buzz_encode(x) for x in [10, 15, 45, 34]]

# create training data
trX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, 2 ** NUM_DIGITS)])
trY = np.array([fizz_buzz_encode(i) for i in range(101, 2 ** NUM_DIGITS)])

# init weigths & bias
NUM_HIDDEN_1 = 10000
NUM_HIDDEN_2 = 1000

weights = {
    'h1': tf.Variable(tf.random_normal([NUM_DIGITS, NUM_HIDDEN_1])),
    'h2': tf.Variable(tf.random_normal([NUM_HIDDEN_1, NUM_HIDDEN_2])),
    'out': tf.Variable(tf.random_normal([NUM_HIDDEN_2, 4]))
}
biases = {
    'b1': tf.Variable(tf.random_normal([NUM_HIDDEN_1])),
    'b2': tf.Variable(tf.random_normal([NUM_HIDDEN_2])),
    'out': tf.Variable(tf.random_normal([4]))
}

# multilayer perceptron
def model(x):
    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])
    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']
    return out_layer

# input has length 10, target 4
X = tf.placeholder("float", [None, NUM_DIGITS])
Y = tf.placeholder("float", [None, 4])

# predict
py_x = model(X)

# cost function.
LEARNING_RATE = 0.05

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)
train_op = optimizer.minimize(cost)

# make predictions using largest output.
predict_op = tf.argmax(py_x, 1)

# get those labels
def fizz_buzz(i, prediction):
    return [str(i), "fizz", "buzz", "fizzbuzz"][prediction]

# params
NUM_EPOCHS = 1000
BATCH_SIZE = 128

# init session
sess = tf.Session()

with sess.as_default():
    tf.global_variables_initializer().run()

    # Training cycle
    for epoch in range(NUM_EPOCHS):
        avg_cost = 0.
        p = np.random.permutation(range(len(trX)))
        trX, trY = trX[p], trY[p]
        # Loop over all batches
        for start in range(0, len(trX), BATCH_SIZE):
            end = start + BATCH_SIZE
            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})

        accuracy = np.mean(np.argmax(trY, axis=1) == sess.run(predict_op, feed_dict={X: trX, Y: trY}))

        if epoch % 100 == 0:
            print("Epoch:", '%04d' % (epoch+1), "acc={:.9f}".format(accuracy))

# alrighty, letz fizzzbuzz
numbers = np.arange(1, 101)
t = np.transpose(binary_encode(numbers, NUM_DIGITS))

res = sess.run(predict_op, feed_dict={X: t})
out = np.vectorize(fizz_buzz)(numbers, res)
out

# mhm definitely needs more layers...
